---
layout: post
title:  "자연어 처리"
author: "DK-Lite"
---

# 자연어처리 시작전

현재 나는 AI 전문가 과정을 듣고있다. 아무래도 가장 좋은 공부법은 누군가를 가르쳐보는게 \
좋은 학습이라 생각되어 이글을 누군가 보고 공부한다는 생각으로 NLP 관련 기술들을 기록해보고자 한자. \
\
기존에 자연어 처리라고 한다면 아래와 같은 프로세스로 진행되어왔다. 
1. Input 
2. Feature Engineering
3. ML Model
4. Ouput

위 Flow중에서 가장 중요한 부분을 뽑자면 아마도 Feature Engineering 이라고 생각된다. \
Feature  Engineering이란 ML에 들어갈 입력 부분을 선정하는 작업인데 \
방대한 자연어 데이터를 아무런 정제없이 무작정 넣었다간 좋은 학습 성과를 낼수 없다. \
그래서 ML에 입력으로 들어가는 부분은 좋은 Feature들로 뽑아 넣어주면 과거에서부터 경험적으로 좋은 \
인식률을 나타내었다 이런 Feature를 추출해내는 방법은 기존에 룰 기반으로 사용해왔다. \

한글을 기준으로 "나는 자연어처리를 좋아한다." 라는 입력을 ML 모델에 넣고자 할때 \
단어들을 기계가 알 수 있도록 표현을 해야한다. \
가장 쉬운 방법으로는 각단어를 표현하는 One-hot Encoding 으로 한다면 이해하기 쉽다. \
> **One-hot Encoding:** 여러개의 입력이 들어갈때 하나의 입력만이 1 나머지는 0이 되는 Encoding 방식 \

그렇다면 각 단어를 선별해 낼까? 아래와 같은 다양한 방법이 있을 것이다.
- ["나는", "자연어처리를", "좋아한다".]
- ["나", "자연어처리", "좋아한다", "."]
- ["나", "자연어", "처리", 좋아", "##한다", "."]

등등 다양한 방법들로서 표현이 되는데 문장에서 이러한 단어들을 뽑아내는 방식을 룰기반으로 많이 사용된다.

이제 이렇게 뽑아낸 단어가 어떻게 인식을 하는냐는 기존에는 확률적으로 많이 접근되었다.
예를들어 나라는 단어와 자연어처리를 단어가 어느정도 등장하느냐 가깝게 있는냐에 따라
다양한 문제 등을 해결할수있게 된다.

하지만 이제는 이러한 단어를 기계화 시키는 부분마저 ML을 이용하고 있다.





# Grouping NPL Probleums in ML View


Classification의 문제는 주어진 인풋에서 하나의 정답 레이블을 찾아내는 분류 문제이다.
과거에는 SVM이 가장 좋은 알고리즘으로 떠올라지만 Deep Learning 의 등장으로
사장 되어 가던 NN이 성장함으로서 SVM의 사용은 줄어들게 되었다.

자연어처리 쪽에서는 
1. Classification 
    - 과거에는 SVM이 가장 좋은 알고리즘

2. Sequential Labeling
    - 자연어처리 쪽에서는 많이 쓰임
    - BIO Tag : 후보-> PLO방식을 한단계로 해결해보자고 나온것

    |B(person)|I(person)|O|
    |---------|---------|-|
    |스티브|잡스|가|

3. Seq 2 Seq
    - 한글하고 영어 단어 갯수가 다름
    - 어떤한 정보를 함축적으로 만들면 인코딩, 반대로 풀면 디코딩
    - Attention 개념

4. N2Structure Probleum

# DNN for NLP
- Previous ML techniques
- input -> Feature Engineering -> ML model -> Output
- Feature Enginneering 어떻게 뽑고 수치화 할것인가 Vector 표현으로 할것인가
- Vector 차원수를 결정 ->
- DNN
- FE이 필요없어짐
- Vector를 어떻게 만드는가?
- Word Embedding을 통해서 만들어냄
- 각 단어의 벡터를 만들어냄
- 기존에 각단어가 실수값을 가졌다면, 이제는 각 단어가 한 벡터를 가지게 된다.
> SOTA : 제일 좋은 성능

# Learing Word Representation For NLP
- No handcraft Feature Engineering !!
- Word Embedding
- Feature를 직접 뽑아 내는 작업을 안해도 된다.
- Dense and Distributed Repesentation
- 기존에는 one-hot으로 표현되는 부분을 Vector화
- Dense : 차원이 확 준다.
- one-hot repesentation의 문제점
    1. 각 디멘저 마다 한 단어를 매핑하기에 많은 인풋 노드가 필요 : 일정한 크기로 Dense
    > 모든 단어의 관계(유사도)는 Vector의 내적으로 계산
    2. 의미적으로도 내적을 계산하면 같거나 0이 되기에 좋지 못하다
    > bag of words 앞이나 뒤 단어들을 묶어서 관리 ( 몇번 나온거만 보겠다.) \
    Unigram : 단어 하나만 가지고 \
    bigram : 내 앞에 단어

- distributed Repersentation
- 유사한 것들끼리 그룹핑이 가능

# Approaches for word Embedding
- Main idea : langueage Modeling
어떤 문장이 제대로 된 문장인지 알고 싶어함 \
앞에 단어가 주어졌을때 그 단어가 나올 확률 
- 대용량의 문서를 가져옴(신문기사) - 제대로 되었다고생각함
- language model을 이용하여 Word Embedding을 함
- 윈도우 사이즈가 비슷한 패턴을 가지도록 학습
- 의미적으로 비슷한 단어들이 같은 문장에 쓰임
- [Word2Vec]
- 주변 단어가 인풋, 주변단어가 아웃풋 : Skip-gram , CBOW
> hidden layer 가 워드임베딩의 dimension
> Pretraing 개념 : unsupervised(LM) - 사람이 태깅할 일이 없음
> Fine Tuning : supervised Learning (NER, SRL, POST) - 세세하게 튜닝한다.

# Convolution NN
- Computer vision에서 많이 사용
- 마지막 MLP를 위해서 
- 신경망의 단점
    - 아키테처가 바뀌면 안됨
    - 이미지는 항상 같을수가 없음
    - 이미지의 중요한 피처를 뽑아서 마지막 MLP인풋으로 맞추는 작업
- What is convolution
- Sliding window function applied to matrix
> 신경망의 쓰는 사람들의 가정 : 파라메터가 많을수록 좋을것라고 생각
- What is max-pooling
- 최대값을 만들어버림 : 사람도 이미지를 볼때 모든것을 보는것이 아닌 각각의 특징을 보고 구분한다.
- 그래서 큰값만 가지고가고 문제가 없다.
- CNN For NLP

# RNN
- 단어의 갯수가 동적이기 때문에 필요
- 하나의 인풋이 들어왔을때
- 히든이 다음 셀타임 인풋에 추가된다.
- RNN 의 단점
    - Vanishing Gradient Probleum
- LSTM
- forget, input, output gate
- gate가 sigmoid 인 이유 : 0~1의 값을 통해서 확률적인 수치로 계산
- LSTM 에서 Gate가 하나 빠져도 크게 문제가 되지 않음
- GRU : LSTM 보다 빠름

- Bert는 Base 모델은 12개 쌓아둠
- Transform을 이해하면 된다.

# NMT
> 자연어 처리는 데이터가 가장 많이 필요하다.
- 한국말 x 영어 데이터는 구하기가 쉽지 않다.
- 세종 코퍼스 (  몇백억 짜리 과제)
- 교차 언어 검색기
- 신경망은 메모라이징 하는 기능이 있다.
- 레이어를 많이 쌓으면 좋은이유 : 내가 기억할수 있는 패턴이 훨씬 많아진다. \
(Relu, pretraining) 하지만 만약에 데이터가 적으면 오버피팅이 된다

- 영어와 한글의 위치간에 매핑을 만드는 작업을 alignment라 한다.
- Blue : 평가 레저
- Source ( 한글), target(영어), dst(영어)
- SOS(State of sentence)
- EOS(End of sentence)
- Seq 2 Seq 단점
    - 앞에 LSTM이 끝나야 다음이 돌고, 인코더와 디코더의 다른 LSTM이 돌기 때문에 느리다 \ (분산처리가 안된다.)
    - 그래서 Transformer model 이 나옴 (분산처리가 가능)
    - vocab 사이즈 만큼 아웃풋이 나온다.
    - 인식에서 동사가 중요한데 한국의 동사가 끝에 있고, 영어는 앞쪽에 있어서 그나마 잘 전달이 되지만 \
    반대로 라틴 -> 한국은 거의 끝에서 끝이기 떄문에 잘 전달이  안된다. (어순을 바꿔서 하는 방법으로 \
    성능를 개선시킨 모델이 있음) - 1
    - Real Time System으로는 부적합

- Variant Decoder(Greedy Encoder)
- Greedy 단점 : 앞 단어를 반영을 하고 있지만 처음부터 끝까지 반영이 안될 수도 있다.
- 그래서 나온것인 Beam Search Decoding : 출력시 가장 큰값을 아웃풋으로 한개 나오는 것이 아닌 \
soft max의 결과의 높은 값을 스퀀셜 마다 2개를 뽑아서 트리 구조로 문장들을 뽑는다 \
( 그리고 결과는 맨뒤부터 뒤로 찾아감)
- Attention Mechanism (Encoder-Decoder Attention)
- 각각의 단어가 순서대로 번역이 되는것이 아니기 때문
- 1번의 문제점을 일반화 시킨것이 Attention
- 히든의 크기가 같으면 유사도 계산이 가능(내적)
- Elemental rise Sum ( 각 Dim 끼리 더함)
- Basic Attention mechanism
- 유사도 계산하는 방법이 필요함
- concat 방법은 MLP를 이용한 아웃풋이 1개인 것인데 사용하는 이유는 파라메터가 많아지기 때문에
- Bahdanau Attention
- Decoder의 히든이 pivot 값인데 이전에 나온 Hidden값으로 계산
- C값으로 들어가는 부분이 Encoder의 Attention값으로 들어간다.
- Luong Attention
- 일반적인 Attention 방식

> Softmax를 쓰는 이유 \
정답을 가지고 Cost Function을 만들려고함 \
그 중 하나로 크로스 엔트로피(확률) \
확률 형싱으로 만들어야만 엔트로피가 계산이 됨 \
또한 tanh를 만들면 음수가 나올수 있음 그래서 Exp추가함

# Attention Is All you need(self Attention)
- 단점 :
    - 학습은 빠른데 테스트가 느리다 ( 인코딩은 한꺼번에 들어오는데 Decoder는 아니다.)
    - 학습 할때는 병렬처리가 되는데 디코딩은 안된다.
- Encoder 
    - Feed Forward는 레이어가 하나 있다고 생각하면 됨.
    - Dimension을 맞추고 싶은 이유가 있음.

- Positional Encoding
    - 단어의 위치 정보가 중요 위치정보에 따라 인코딩 정보가 같은게 문제
    - 위치에 따라서 일정한 패턴을 만들겠다.
    - 위치마다 벡터 패턴을 만들어서 인풋에 반영을 해서 넣기에 위치정보가 스며듦
    - PE값을 각 인풋 벡터에 더 함으로서 임베딩의 위치정보를 넣게된다.
    - 의미적으로는 곱해야한다. 하지만 워드임베딩 값이 너무 달라져서 더한다.

- Residual Connection
    - 이미지 프로세싱에서 나온 용어, Layer가 너무 많으면 인풋의 특징이 위로 안올라감 \
    무언가 레이어가 거쳐지면 인풋ㄱ밧을 다시 불어 넣어줌
    - 원래 값을 아웃풋에 EWS을 함.  더해지기 때문에 다시 정규화

- Multi Head Attention
    - self-attention : 자기들끼리 유사도를 계산함
    - 한문서가 100개의 단어로 이루어져있을때 \
    다른 단어들간의 유사도의 섬이 가장 높은 단어가 가장 Topical Word이다
    - MaskedDecoder self-attention : 앞에 있는 애들과의 관계가 있다고 생각하여 Attention을 계산
    - Attention(Q, K, V)
    - 기준이 되는 벡터를 Query 라고 함
    - 유사도를 계산하는 것들을 key
    > 결국 Attention 계산은 Encoder-Decoder 모델에서 Attetion Output을 출력하는 부분 \
    에 해당 Query가 Decoder에서 나오는 것이냐 아니면 자기 자신이냐 차이
    - Dk는 nomalize vecotor(K Dimension)
    - Scale dot-product Attention
    - 파라메터를 늘리기 위해서 필요
    - QKV를 다르게 하고 싶은
    - CNN Filter와 비슷
    - Multi Head의 의미를 filter라고 생각하면됨
    - QKV를 각각 다르게 만듦
    - Decoding 한단어씩 해야함
    - 학습을 할대는 빠르게 가능 :  이미 맞는 답을 셋팅하기에 상관없다. (원리 이해 필요?)

    > byteNet \
    CNN은 병력 처리가 가능 RNN은 불가능 \
    ByteNet은 CNN으로 하자가 핵심인데 트랜스포머 모델의 등장으로 망함

# Introduction of dialogue system
- 내 다음의 발화를 무엇으로 할것인가 대화 정책관리
- 그말을 만들어 내는게 NLG
- 슬로ㅓㅅ이라는 것은 DB의 Attribute라고 생각
- 각 발화에서 슬랏을 겟해야함
- DM은 질문에 대한 답변을 만들어내야함
- 어느정도 DM에 정보를 줘야함

# 목적 지향 대화 시스템
- Dialogue State Tracker
- 확률값으로 나옴
- 요즘에는 RL로 함
- DM output
- Seq2Seq로도 가능 \
ex) Seggest(name='the...) -> #name 어떻습니까?
- 자연어를 화행으로 변환하는것
- 화행은 이미 결정되어있다. CNN ㅢㅖ
- 슬롯 필링은 Sequential
- 화행 분석
- 문장이 들어오면 Feature 뽑아서
- Template, Seq2Seq 기반
> 화행 : 요청한 사람의 의도를 파악 \
슬롯필링 : 개체명을 찾는것(DB검색을 휘나 필요한 정보 Attribute들을 찾아내는것)

    

 